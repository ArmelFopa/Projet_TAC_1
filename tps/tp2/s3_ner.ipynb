{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance d'entités nommées avec SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation et chargement du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers trouvés pour 1950: 4\n",
      "Taille du corpus 1950: 133,895 caractères\n",
      "\n",
      "Extrait:\n",
      " L'AVENIR DU LUXEMBOURG Samedi 15 avri j 350, \n",
      "MORHET \n",
      "Soirée dramatique \n",
      "1 Le cercle dramatique Sainte-Cécile \n",
      "de Morhet reprendra, ce dimanche 16 \n",
      "avril ^Quasimodo), sa brillante soirée \n",
      "qui a remporté un succès si remarqua-\n",
      "| bie le 10 mars dernier. \n",
      "i Rappelons ie programme : \n",
      "; 1) ouverture : « Brabançonne »,par \n",
      "• la Fantare ; 2) « La .bohème », chœur \n",
      "à 2 voix exécuté par JV^.es Renée Cara, \n",
      "j Josée Goffin, Anyse Hubermont et Hé-\n",
      "f lène Bellanger ; a) La comédie en deux \n",
      "actes de Marcell* \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "data_dir = Path(\"../../data/txt\")\n",
    "\n",
    "all_txt = list(data_dir.glob(\"*.txt\"))\n",
    "\n",
    "YEAR = \"1950\"\n",
    "pat = re.compile(rf\"{YEAR}\")\n",
    "\n",
    "files_year = [p for p in all_txt if pat.search(p.name)]\n",
    "print(f\"Fichiers trouvés pour {YEAR}: {len(files_year)}\")\n",
    "\n",
    "corpus_year = \"\"\n",
    "for path in files_year:\n",
    "    for enc in (\"utf-8\", \"latin-1\", \"cp1252\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc, errors=\"ignore\") as f:\n",
    "                corpus_year += f.read() + \"\\n\"\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "print(f\"Taille du corpus {YEAR}: {len(corpus_year):,} caractères\")\n",
    "print(\"\\nExtrait:\\n\", corpus_year[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La documentation est accessible ici: https://spacy.io/api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 2.1/16.3 MB 10.7 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 4.5/16.3 MB 11.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 6.8/16.3 MB 11.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 9.2/16.3 MB 11.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 11.8/16.3 MB 11.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 14.4/16.3 MB 11.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  16.3/16.3 MB 11.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 16.3/16.3 MB 10.9 MB/s  0:00:01\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"fr_core_news_lg\")\n",
    "except OSError:\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse d'un petit extrait du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130219"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ner = re.sub(r\"-\\s*\\n\\s*\", \"\", corpus_year)\n",
    "text_ner = re.sub(r\"\\s+\\n\", \"\\n\", text_ner)\n",
    "text_ner = re.sub(r\"\\n{2,}\", \"\\n\\n\", text_ner)\n",
    "len(text_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831,\n",
       " [('AVENIR', 'ORG'),\n",
       "  ('Samedi', 'PER'),\n",
       "  ('Soirée', 'LOC'),\n",
       "  ('Sainte-Cécile', 'LOC'),\n",
       "  ('Morhet', 'PER'),\n",
       "  ('Brabançonne', 'LOC'),\n",
       "  ('JV^.es Renée Cara', 'PER'),\n",
       "  ('j Josée Goffin', 'LOC'),\n",
       "  ('Héf lène Bellanger', 'PER'),\n",
       "  ('Marcell', 'LOC')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text_ner[:50000])\n",
    "entities = [(e.text.strip(), e.label_) for e in doc.ents]\n",
    "len(entities), entities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection et comptage des entités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2744,\n",
       " [('AVENIR', 383),\n",
       "  ('Samedi', 4317129024397789502),\n",
       "  ('Soirée', 385),\n",
       "  ('Sainte-Cécile', 385),\n",
       "  ('Morhet', 4317129024397789502),\n",
       "  ('Brabançonne', 385),\n",
       "  ('JV^.es Renée Cara', 4317129024397789502),\n",
       "  ('j Josée Goffin', 385),\n",
       "  ('Héf lène Bellanger', 4317129024397789502),\n",
       "  ('Marcell', 385)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def chunk(s, size=10000):\n",
    "    for i in range(0, len(s), size):\n",
    "        yield s[i:i+size]\n",
    "\n",
    "entities = []\n",
    "for piece in chunk(text_ner, 10000):\n",
    "    d = nlp(piece)\n",
    "    for e in d.ents:\n",
    "        entities.append((e.text.strip(), e.label))\n",
    "len(entities), entities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2744,\n",
       " [('AVENIR', 'ORG'),\n",
       "  ('Samedi', 'PER'),\n",
       "  ('Soirée', 'LOC'),\n",
       "  ('Sainte-Cécile', 'LOC'),\n",
       "  ('Morhet', 'PER')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = []\n",
    "for piece in (text_ner[i:i+10000] for i in range(0, len(text_ner), 10000)):\n",
    "    d = nlp(piece)\n",
    "    for e in d.ents:\n",
    "        entities.append((e.text.strip().replace(\"\\n\", \"\"), e.label_))\n",
    "len(entities), entities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2490"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def keep(txt):\n",
    "    if len(txt) < 3: \n",
    "        return False\n",
    "    if re.search(r\"[~^@&%$<>]\", txt): \n",
    "        return False\n",
    "    if sum(ch.isdigit() for ch in txt) > 0: \n",
    "        return False\n",
    "    return True\n",
    "\n",
    "entities_clean = [(t, lab) for (t, lab) in entities if keep(t)]\n",
    "len(entities_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top PER : [('Jambes', 15), ('Harold', 9), ('Oui', 7), ('Jeunes', 7), ('Phyllis', 7), ('Van Zeeland', 6), ('Roi', 6), ('Robert', 5), ('Jumet', 5), ('Mme de Sermaize', 4)]\n",
      "Top LOC : [('Bruxelles', 27), ('Namur', 27), ('Belgique', 8), ('Etat', 8), ('Vieux', 8), ('Belga', 7), ('Paris', 6), ('Londres', 6), ('Charleroi', 6), ('Pologne', 6)]\n",
      "Top ORG : [('NAMUR', 8), ('PRODENT', 5), ('BRUX', 4), ('XXV', 3), ('JAMBES', 3), ('Ford', 3), ('PAYS', 2), (\"ministre de l'Intérieur\", 2), ('Sénat', 2), ('Chambre', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "top_per = Counter([t for t, lab in entities_clean if lab == \"PER\"]).most_common(10)\n",
    "top_loc = Counter([t for t, lab in entities_clean if lab == \"LOC\"]).most_common(10)\n",
    "top_org = Counter([t for t, lab in entities_clean if lab == \"ORG\"]).most_common(10)\n",
    "\n",
    "print(\"Top PER :\", top_per[:10])\n",
    "print(\"Top LOC :\", top_loc[:10])\n",
    "print(\"Top ORG :\", top_org[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple sur un corpus de test fourni par SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le corpus de Spacy\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isoler la première phrase\n",
    "sent = sentences[0]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traiter la phrase avec Spacy\n",
    "doc = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer le test sur toutes les phrases\n",
    "for sent in sentences:\n",
    "    doc = nlp(sent)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append(f\"{ent.text} ({ent.label_})\")\n",
    "    if entities:\n",
    "        print(f\"'{doc.text}' contient les entités suivantes : {', '.join(entities)}\")\n",
    "    else:\n",
    "        print(f\"'{doc.text}' ne contient aucune entité\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquer la reconnaissance d'entités nommées sur notre corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=1000000\n",
    "text = open(\"../data/all.txt\", encoding='utf-8').read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice: essayez de lister les lieux (LOC) et les organisations (ORG) les plus mentionnées dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROJET_TAC_1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
